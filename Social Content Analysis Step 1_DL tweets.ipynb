{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This notebook will download tweets from authors indicated by a list including their Twitter handle, up to ~3200 tweets per brand (max allowed by the Twitter API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import csv\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Load developer credentials for Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2514"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('credentials.json') as creds_file:\n",
    "    credentials = json.load(creds_file)\n",
    "    \n",
    "consumer_key = credentials['consumer_key']\n",
    "consumer_secret = credentials['consumer_secret']\n",
    "access_key = credentials['access_key']\n",
    "access_secret = credentials['access_secret']\n",
    "\n",
    "#authorize twitter, initialize tweepy\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "api.get_user(screen_name='aliss77777').statuses_count"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Before you run this function, create a folder 'exports' in your local directory to capture the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to download the tweets\n",
    "def get_all_tweets(screen_name):\n",
    "\n",
    "\tpath = 'exports/' # folder to save the files to. make sure to create this folder in advance so you don't get an error\n",
    "\n",
    "\t#authorize twitter, initialize tweepy\n",
    "\tauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "\tauth.set_access_token(access_key, access_secret)\n",
    "\tapi = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "\t#initialize a list to hold all the tweepy Tweets & list with no retweets\n",
    "\talltweets = []\n",
    "\tnoRT = []\n",
    "\n",
    "\t#get count of users total lifetime tweets\n",
    "\tlifetime_tweets = api.get_user(screen_name = screen_name).statuses_count\n",
    "    \n",
    "    #make initial request for most recent tweets with extended mode enabled to get full tweets\n",
    "\tnew_tweets = api.user_timeline(screen_name = screen_name, tweet_mode = 'extended', count=200)\n",
    "\n",
    "\t#save most recent tweets\n",
    "\talltweets.extend(new_tweets)\n",
    "\n",
    "\t#save the id of the oldest tweet less one\n",
    "\toldest = alltweets[-1].id - 1\n",
    "\n",
    "\t# figuring out the stop value for the loop: lessor of 3200 (API rate limit) or the users total number of lifetime tweets\n",
    "\tstop_value = min(3020, lifetime_tweets-75) # an arbitrary buffer based on cases where it was hanging b/c of off by around 10-40 tweets\n",
    "    \n",
    "    #keep grabbing tweets until the api limit is reached\n",
    "\t#while len(alltweets) <= 3200:\n",
    "\twhile len(alltweets) <= stop_value:\n",
    "\t\tprint(\"getting tweets before {}\".format(oldest))\n",
    "\n",
    "\t\t#all subsiquent requests use the max_id param to prevent duplicates\n",
    "\t\tnew_tweets = api.user_timeline(screen_name = screen_name, tweet_mode = 'extended', count=200, max_id=oldest)\n",
    "\n",
    "\t\t#save most recent tweets\n",
    "\t\talltweets.extend(new_tweets)\n",
    "\n",
    "\t\t#update the id of the oldest tweet less one\n",
    "\t\toldest = alltweets[-1].id - 1\n",
    "\n",
    "\t\tprint(\"...{} tweets downloaded so far\".format(len(alltweets)))\n",
    "\n",
    "\t\t#removes retweets\n",
    "\tfor tweet in alltweets:\n",
    "\t\tif 'RT' in tweet.full_text:\n",
    "\t\t\tcontinue\n",
    "\t\telse:\n",
    "\t\t\tnoRT.append([tweet.id_str, tweet.created_at, tweet.full_text])\n",
    "\n",
    "\t#write to csv\n",
    "\twith open(path+'{}_tweets.csv'.format(screen_name), 'w') as f:\n",
    "\t\twriter = csv.writer(f)\n",
    "\t\twriter.writerow([\"id\",\"created_at\",\"text\"])\n",
    "\t\twriter.writerows(noRT)\n",
    "\t\tprint('{}_tweets.csv was successfully created.'.format(screen_name))\n",
    "\tpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creds to test API through Tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually creating a list of acccounts to DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_accounts = [\n",
    "    '@RESCUEorg',\n",
    "    '@SavetheChildren',\n",
    "    '@RedCross',\n",
    "    '@WCKitchen',\n",
    "    '@GlobalGiving',\n",
    "    '@UNICEF',\n",
    "    '@ICRC',\n",
    "    '@MSF',\n",
    "    '@UNHumanRights',\n",
    "    '@Refugees'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 1482625419543207938\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 1462423368574152707\n",
      "...598 tweets downloaded so far\n",
      "getting tweets before 1441059925787127807\n",
      "...798 tweets downloaded so far\n",
      "getting tweets before 1424819670146682880\n",
      "...998 tweets downloaded so far\n",
      "getting tweets before 1406664231370465279\n",
      "...1197 tweets downloaded so far\n",
      "getting tweets before 1394710386398568451\n",
      "...1397 tweets downloaded so far\n",
      "getting tweets before 1383092333810315269\n",
      "...1597 tweets downloaded so far\n",
      "getting tweets before 1375085768905986049\n",
      "...1797 tweets downloaded so far\n",
      "getting tweets before 1367909032695107587\n",
      "...1997 tweets downloaded so far\n",
      "getting tweets before 1358451627242500095\n",
      "...2197 tweets downloaded so far\n",
      "getting tweets before 1347227289600225279\n",
      "...2397 tweets downloaded so far\n",
      "getting tweets before 1333812954207195135\n",
      "...2597 tweets downloaded so far\n",
      "getting tweets before 1321506411864489984\n",
      "...2797 tweets downloaded so far\n",
      "getting tweets before 1311373480831582207\n",
      "...2997 tweets downloaded so far\n",
      "getting tweets before 1302806150560313343\n",
      "...3196 tweets downloaded so far\n",
      "@RESCUEorg_tweets.csv was successfully created.\n",
      "getting tweets before 1476994066642518015\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1454132107761041411\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1430922916305281025\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1409604611929956352\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1389264759023882239\n",
      "...1200 tweets downloaded so far\n",
      "getting tweets before 1369669984079740931\n",
      "...1400 tweets downloaded so far\n",
      "getting tweets before 1347739188762976255\n",
      "...1600 tweets downloaded so far\n",
      "getting tweets before 1329878447737417730\n",
      "...1800 tweets downloaded so far\n",
      "getting tweets before 1310557650078310403\n",
      "...2000 tweets downloaded so far\n",
      "getting tweets before 1291743224911081471\n",
      "...2199 tweets downloaded so far\n",
      "getting tweets before 1274313732798050303\n",
      "...2399 tweets downloaded so far\n",
      "getting tweets before 1264937667210489856\n",
      "...2599 tweets downloaded so far\n",
      "getting tweets before 1253732250123894784\n",
      "...2799 tweets downloaded so far\n",
      "getting tweets before 1243607029220085762\n",
      "...2999 tweets downloaded so far\n",
      "getting tweets before 1234095263003873280\n",
      "...3199 tweets downloaded so far\n",
      "@SavetheChildren_tweets.csv was successfully created.\n",
      "getting tweets before 1486055621011124229\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 1471921876918226943\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 1455633626117312512\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 1438170754953068544\n",
      "...999 tweets downloaded so far\n",
      "getting tweets before 1423735759102783493\n",
      "...1199 tweets downloaded so far\n",
      "getting tweets before 1410253581580652552\n",
      "...1399 tweets downloaded so far\n",
      "getting tweets before 1395448890862809090\n",
      "...1599 tweets downloaded so far\n",
      "getting tweets before 1379205593085444106\n",
      "...1799 tweets downloaded so far\n",
      "getting tweets before 1367869921011658753\n",
      "...1999 tweets downloaded so far\n",
      "getting tweets before 1357502691073785866\n",
      "...2199 tweets downloaded so far\n",
      "getting tweets before 1341490807556235264\n",
      "...2399 tweets downloaded so far\n",
      "getting tweets before 1329544989387452415\n",
      "...2599 tweets downloaded so far\n",
      "getting tweets before 1319060740289023999\n",
      "...2799 tweets downloaded so far\n",
      "getting tweets before 1308142137494704127\n",
      "...2999 tweets downloaded so far\n",
      "getting tweets before 1299361729923747842\n",
      "...3199 tweets downloaded so far\n",
      "@RedCross_tweets.csv was successfully created.\n",
      "getting tweets before 1497895201003687935\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1494344102376263685\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1469856007518097412\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1437803007161389065\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1429146258300719110\n",
      "...1200 tweets downloaded so far\n",
      "getting tweets before 1407722978796736513\n",
      "...1400 tweets downloaded so far\n",
      "getting tweets before 1391069738114105348\n",
      "...1600 tweets downloaded so far\n",
      "getting tweets before 1375558780696547329\n",
      "...1800 tweets downloaded so far\n",
      "getting tweets before 1363264322915487743\n",
      "...2000 tweets downloaded so far\n",
      "getting tweets before 1350890948918710271\n",
      "...2200 tweets downloaded so far\n",
      "getting tweets before 1338972477192613888\n",
      "...2400 tweets downloaded so far\n",
      "getting tweets before 1329973478590738433\n",
      "...2600 tweets downloaded so far\n",
      "getting tweets before 1322653641891848191\n",
      "...2800 tweets downloaded so far\n",
      "getting tweets before 1317651074250047487\n",
      "...3000 tweets downloaded so far\n",
      "getting tweets before 1311796117747765249\n",
      "...3200 tweets downloaded so far\n",
      "@WCKitchen_tweets.csv was successfully created.\n",
      "getting tweets before 1478464502404366342\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 1456226470662184961\n",
      "...599 tweets downloaded so far\n",
      "getting tweets before 1432024749903949823\n",
      "...799 tweets downloaded so far\n",
      "getting tweets before 1413560088162443265\n",
      "...999 tweets downloaded so far\n",
      "getting tweets before 1394703463989145601\n",
      "...1199 tweets downloaded so far\n",
      "getting tweets before 1386097628694392833\n",
      "...1399 tweets downloaded so far\n",
      "getting tweets before 1368302370715746303\n",
      "...1599 tweets downloaded so far\n",
      "getting tweets before 1352678427397148672\n",
      "...1798 tweets downloaded so far\n",
      "getting tweets before 1333444710220902399\n",
      "...1998 tweets downloaded so far\n",
      "getting tweets before 1320720843371106306\n",
      "...2198 tweets downloaded so far\n",
      "getting tweets before 1306979052855664639\n",
      "...2398 tweets downloaded so far\n",
      "getting tweets before 1298686393946968065\n",
      "...2598 tweets downloaded so far\n",
      "getting tweets before 1295713956871446529\n",
      "...2798 tweets downloaded so far\n",
      "getting tweets before 1285230742671372287\n",
      "...2998 tweets downloaded so far\n",
      "getting tweets before 1271525171225034753\n",
      "...3198 tweets downloaded so far\n",
      "@GlobalGiving_tweets.csv was successfully created.\n",
      "getting tweets before 1496288918312001537\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1488346809575608325\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1478773354748329986\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1469110876770816004\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1462108761158471684\n",
      "...1200 tweets downloaded so far\n",
      "getting tweets before 1459477390678130688\n",
      "...1400 tweets downloaded so far\n",
      "getting tweets before 1454432909067100160\n",
      "...1600 tweets downloaded so far\n",
      "getting tweets before 1446865379951792127\n",
      "...1800 tweets downloaded so far\n",
      "getting tweets before 1440329563414806537\n",
      "...2000 tweets downloaded so far\n",
      "getting tweets before 1430383180868055039\n",
      "...2200 tweets downloaded so far\n",
      "getting tweets before 1421122736299065345\n",
      "...2399 tweets downloaded so far\n",
      "getting tweets before 1410995430234628096\n",
      "...2599 tweets downloaded so far\n",
      "getting tweets before 1402512660852269056\n",
      "...2799 tweets downloaded so far\n",
      "getting tweets before 1395532302512427009\n",
      "...2999 tweets downloaded so far\n",
      "getting tweets before 1387780850234822656\n",
      "...3199 tweets downloaded so far\n",
      "@UNICEF_tweets.csv was successfully created.\n",
      "getting tweets before 1492352046791634943\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1477535291908972548\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1463160473805901826\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1449427939993477130\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1437718408741675016\n",
      "...1200 tweets downloaded so far\n",
      "getting tweets before 1427638608526290951\n",
      "...1400 tweets downloaded so far\n",
      "getting tweets before 1414871531231105024\n",
      "...1600 tweets downloaded so far\n",
      "getting tweets before 1401104318690955264\n",
      "...1800 tweets downloaded so far\n",
      "getting tweets before 1390264392495869951\n",
      "...2000 tweets downloaded so far\n",
      "getting tweets before 1375360516244275204\n",
      "...2200 tweets downloaded so far\n",
      "getting tweets before 1359461175398965247\n",
      "...2400 tweets downloaded so far\n",
      "getting tweets before 1339178902577680383\n",
      "...2600 tweets downloaded so far\n",
      "getting tweets before 1325053986068598783\n",
      "...2800 tweets downloaded so far\n",
      "getting tweets before 1311982052049399808\n",
      "...3000 tweets downloaded so far\n",
      "getting tweets before 1298254588785483775\n",
      "...3200 tweets downloaded so far\n",
      "@ICRC_tweets.csv was successfully created.\n",
      "getting tweets before 1487053848413757439\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1473327370425745411\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1463912866604531720\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1455109079270035459\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1444872655602081791\n",
      "...1200 tweets downloaded so far\n",
      "getting tweets before 1430170062346928143\n",
      "...1400 tweets downloaded so far\n",
      "getting tweets before 1420796260286550019\n",
      "...1600 tweets downloaded so far\n",
      "getting tweets before 1407623964805918719\n",
      "...1800 tweets downloaded so far\n",
      "getting tweets before 1395032315936575487\n",
      "...2000 tweets downloaded so far\n",
      "getting tweets before 1384133901434507264\n",
      "...2200 tweets downloaded so far\n",
      "getting tweets before 1371870937184931848\n",
      "...2400 tweets downloaded so far\n",
      "getting tweets before 1363455879446724611\n",
      "...2600 tweets downloaded so far\n",
      "getting tweets before 1355181053837762563\n",
      "...2800 tweets downloaded so far\n",
      "getting tweets before 1345788263773179908\n",
      "...3000 tweets downloaded so far\n",
      "getting tweets before 1336713888444846079\n",
      "...3200 tweets downloaded so far\n",
      "@MSF_tweets.csv was successfully created.\n",
      "getting tweets before 1470799942964948992\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1455854014629924863\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1434892342205505538\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1410933467379032063\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1393198484443082751\n",
      "...1200 tweets downloaded so far\n",
      "getting tweets before 1372888001127313409\n",
      "...1400 tweets downloaded so far\n",
      "getting tweets before 1362061064909828096\n",
      "...1600 tweets downloaded so far\n",
      "getting tweets before 1339593862864506881\n",
      "...1800 tweets downloaded so far\n",
      "getting tweets before 1336608109066932225\n",
      "...2000 tweets downloaded so far\n",
      "getting tweets before 1327982350198829056\n",
      "...2200 tweets downloaded so far\n",
      "getting tweets before 1309815907384995839\n",
      "...2400 tweets downloaded so far\n",
      "getting tweets before 1293514074836340735\n",
      "...2600 tweets downloaded so far\n",
      "getting tweets before 1277901881101123583\n",
      "...2800 tweets downloaded so far\n",
      "getting tweets before 1268970274415874047\n",
      "...3000 tweets downloaded so far\n",
      "getting tweets before 1260575588848410626\n",
      "...3200 tweets downloaded so far\n",
      "@UNHumanRights_tweets.csv was successfully created.\n",
      "getting tweets before 1495084270947340289\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 1489180361150107650\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 1483087471130943488\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 1476058649617387519\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 1470525188051722242\n",
      "...1200 tweets downloaded so far\n",
      "getting tweets before 1466893764165836803\n",
      "...1400 tweets downloaded so far\n",
      "getting tweets before 1463080896123346946\n",
      "...1600 tweets downloaded so far\n",
      "getting tweets before 1458739811355926534\n",
      "...1800 tweets downloaded so far\n",
      "getting tweets before 1454549955541078022\n",
      "...2000 tweets downloaded so far\n",
      "getting tweets before 1450012532354256898\n",
      "...2200 tweets downloaded so far\n",
      "getting tweets before 1445709437038071814\n",
      "...2400 tweets downloaded so far\n",
      "getting tweets before 1442568481036128258\n",
      "...2600 tweets downloaded so far\n",
      "getting tweets before 1439491699147825155\n",
      "...2800 tweets downloaded so far\n",
      "getting tweets before 1435977047667531778\n",
      "...3000 tweets downloaded so far\n",
      "getting tweets before 1432628727603630090\n",
      "...3200 tweets downloaded so far\n",
      "@Refugees_tweets.csv was successfully created.\n"
     ]
    }
   ],
   "source": [
    "for name in list_of_accounts: \n",
    "    get_all_tweets(screen_name=name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "social_content_analysis",
   "language": "python",
   "name": "social_content_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
